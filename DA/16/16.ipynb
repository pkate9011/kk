{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e7d46c-c10c-4259-814e-914abc36765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from heapq import nlargest\n",
    "# Sample text paragraph you can write any text\n",
    "text = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, informationengineering, and artificial\n",
    "intelligence concerned with the interactions between computers and humanlanguages, in particular how to program computers to process \n",
    "and analyze large amounts of naturallanguage data. Challenges in natural language processing frequently involve speech recognition, \n",
    "naturallanguage understanding, and natural language generation. The history of natural language processinggenerally started in the 1950s,\n",
    "although work can be found from earlier periods.\"\"\"\n",
    "# Remove special characters and digits\n",
    "text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)# Tokenize each sentence into words and remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(word_tokenize(sentence))\n",
    "words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "# Calculate word frequency\n",
    "word_freq = nltk.FreqDist(words)\n",
    "# Calculate sentence scores based on word frequency\n",
    "sentence_scores = {}\n",
    "for sentence in sentences:\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in word_freq:\n",
    "            if len(sentence.split(' ')) < 30:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freq[word]\n",
    "# Generate summary by selecting top 3 sentences with highest scores\n",
    "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab1c1c-196c-4f9e-8608-5fb5c84a9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "    OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba0a25-021e-4bb4-8756-7658887e1a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19370678-9ad2-4716-bdb8-df78d607f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello all Welcome to Python Programming Academy  Python Programming Academy  is a nice platform to learn new programming skills It is difficult to get enrolled in this Academy \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Text to summarize\n",
    "text = \"Hello all, Welcome to Python Programming Academy. 5 Python Programming Academy @ is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy.\"\n",
    "# Preprocess the text to remove special characters and digits\n",
    "preprocessed_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "# Tokenize the preprocessed text into sentences\n",
    "sentences = sent_tokenize(preprocessed_text)# Calculate the importance score of each sentence using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "# Select top N sentences based on their importance score\n",
    "N=2\n",
    "top_sentences = sorted(range(len(similarity_matrix[-1])), key=lambda i: similarity_matrix[-1][i])[-N:]\n",
    "# Concatenate the top sentences to form the summary\n",
    "summary = ''\n",
    "for i in top_sentences:\n",
    "    for i in top_sentences:\n",
    "        summary += sentences[i] + ' '\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48393663-20d4-42b0-8baf-a5e3ef31a331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
